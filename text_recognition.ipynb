{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Reshape, Activation, Input, Flatten, Dense, GRU\n",
    "from keras.optimizers import Adam\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS\n",
    "# function to get unique values \n",
    "def unique(list1): \n",
    "    # intilize a null list \n",
    "    unique_list = [] \n",
    "      \n",
    "    # traverse for all elements \n",
    "    for x in list1: \n",
    "        # check if exists in unique_list or not \n",
    "        if x not in unique_list: \n",
    "            unique_list.append(x) \n",
    "#     # print list \n",
    "#     for x in unique_list: \n",
    "#         print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération des données\n",
    "lines = open(\"./data.csv\", encoding=\"utf8\").readlines()\n",
    "lines = np.array(lines)\n",
    "\n",
    "# Formattage des données, et exclusion des données inutiles\n",
    "lines = [l.split('\"') for l in lines]\n",
    "lines = np.array([[l[3], l[5]] for l in lines])\n",
    "# print(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1328\n"
     ]
    }
   ],
   "source": [
    "# TOKENIZER WORK\n",
    "data_text = [t[1] for t in lines]\n",
    "\n",
    "num_words = 10000\n",
    "# Tokenizer converts words to integers\n",
    "corpus = [sentence for sentence in data_text if sentence.count(' ') >= 2]\n",
    "tokenizer = Tokenizer(num_words)\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "idx = tokenizer.word_index\n",
    "inverse_map = dict(zip(idx.values(), idx.keys()))\n",
    "\n",
    "def tokens_to_string(tokens):\n",
    "    # Map from tokens back to words.\n",
    "    words = [inverse_map[token] for token in tokens if token != 0]\n",
    "    # Concatenate all words.\n",
    "    text = \" \".join(words)\n",
    "    return text\n",
    "\n",
    "# print(x_train[2])\n",
    "# print(tokens_to_string(x_train_pad[2]))\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad = 'pre' #we add zeros to the beginning and also truncating from beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getXPad(x_train, x_test):\n",
    "    x_train_tokens = tokenizer.texts_to_sequences(x_train)\n",
    "    print(x_train_tokens[1])\n",
    "\n",
    "    x_test_tokens = tokenizer.texts_to_sequences(x_test)\n",
    "    #calculating length of every training tokens and converted it to an array\n",
    "    num_tokens = [len(tokens) for tokens in x_train_tokens + x_test_tokens]\n",
    "    num_tokens = np.array(num_tokens)\n",
    "\n",
    "    # # mean length of token in training data\n",
    "    # np.mean(num_tokens)\n",
    "\n",
    "    # #max length of training token data before truncate\n",
    "    # np.max(num_tokens)\n",
    "\n",
    "    #we find max number of tokens we will allow is set to the average plus 2 standard deviations.\n",
    "    max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens)\n",
    "    max_tokens = int(max_tokens)\n",
    "    max_tokens  #maximum token length in our data\n",
    "\n",
    "    # #This covers about 95% of the data-set.\n",
    "    # np.sum(num_tokens < max_tokens) / len(num_tokens)\n",
    "\n",
    "    x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens, padding=pad, truncating=pad)\n",
    "    x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens, padding=pad, truncating=pad)\n",
    "    return x_train_pad, x_test_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, None, 8)           10624     \n",
      "_________________________________________________________________\n",
      "gru_7 (GRU)                  (None, 16)                1200      \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 11,841\n",
      "Trainable params: 11,841\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "----------------------------------------------------\n",
      "---------   Training with VADER :       ----\n",
      "----------------------------------------------------\n",
      "[2, 420, 253, 65, 11, 15, 421, 81, 22, 17, 636, 6, 5, 162, 637, 254, 82, 1, 20, 255, 4, 422]\n",
      "Epoch 1/25\n",
      "337/337 [==============================] - 2s 6ms/step - loss: 0.6338 - acc: 0.9407\n",
      "Epoch 2/25\n",
      "337/337 [==============================] - 1s 2ms/step - loss: 0.4481 - acc: 0.9555\n",
      "Epoch 3/25\n",
      "337/337 [==============================] - 0s 1ms/step - loss: 0.2133 - acc: 0.9555\n",
      "Epoch 4/25\n",
      "337/337 [==============================] - 0s 1ms/step - loss: 0.1819 - acc: 0.9555\n",
      "Epoch 5/25\n",
      "337/337 [==============================] - 0s 1ms/step - loss: 0.1793 - acc: 0.9555\n",
      "Epoch 6/25\n",
      "337/337 [==============================] - 0s 1ms/step - loss: 0.1776 - acc: 0.9555\n",
      "Epoch 7/25\n",
      "337/337 [==============================] - 0s 1ms/step - loss: 0.1727 - acc: 0.9555\n",
      "Epoch 8/25\n",
      "337/337 [==============================] - 1s 2ms/step - loss: 0.1686 - acc: 0.9555\n",
      "Epoch 9/25\n",
      "337/337 [==============================] - 0s 1ms/step - loss: 0.1629 - acc: 0.9555\n",
      "Epoch 10/25\n",
      "337/337 [==============================] - 0s 1ms/step - loss: 0.1537 - acc: 0.9555\n",
      "Epoch 11/25\n",
      "337/337 [==============================] - 1s 2ms/step - loss: 0.1459 - acc: 0.9555\n",
      "Epoch 12/25\n",
      "337/337 [==============================] - 0s 1ms/step - loss: 0.1350 - acc: 0.9555\n",
      "Epoch 13/25\n",
      "337/337 [==============================] - 1s 2ms/step - loss: 0.1227 - acc: 0.9555\n",
      "Epoch 14/25\n",
      "337/337 [==============================] - 0s 1ms/step - loss: 0.1112 - acc: 0.9555\n",
      "Epoch 15/25\n",
      "337/337 [==============================] - 1s 2ms/step - loss: 0.0975 - acc: 0.9555\n",
      "Epoch 16/25\n",
      "337/337 [==============================] - 1s 2ms/step - loss: 0.0858 - acc: 0.9585\n",
      "Epoch 17/25\n",
      "337/337 [==============================] - 1s 2ms/step - loss: 0.0737 - acc: 0.9644\n",
      "Epoch 18/25\n",
      "337/337 [==============================] - 1s 1ms/step - loss: 0.0649 - acc: 0.9703\n",
      "Epoch 19/25\n",
      "337/337 [==============================] - 0s 1ms/step - loss: 0.0561 - acc: 0.9792\n",
      "Epoch 20/25\n",
      "337/337 [==============================] - 0s 1ms/step - loss: 0.0454 - acc: 0.9852\n",
      "Epoch 21/25\n",
      "337/337 [==============================] - 1s 2ms/step - loss: 0.0396 - acc: 0.9852\n",
      "Epoch 22/25\n",
      "337/337 [==============================] - 1s 2ms/step - loss: 0.0348 - acc: 0.9881\n",
      "Epoch 23/25\n",
      "337/337 [==============================] - 1s 1ms/step - loss: 0.0312 - acc: 0.9881\n",
      "Epoch 24/25\n",
      "337/337 [==============================] - 0s 1ms/step - loss: 0.0288 - acc: 0.9911\n",
      "Epoch 25/25\n",
      "337/337 [==============================] - 0s 1ms/step - loss: 0.0236 - acc: 0.9941\n",
      "337/337 [==============================] - 0s 1ms/step\n",
      "Accuracy = 91.69139469412738 %\n",
      "----------------------------------------------------\n",
      "---------   Training with ANAKIN :       ----\n",
      "----------------------------------------------------\n",
      "[2, 420, 253, 65, 11, 15, 421, 81, 22, 17, 636, 6, 5, 162, 637, 254, 82, 1, 20, 255, 4, 422]\n",
      "Epoch 1/25\n",
      "337/337 [==============================] - 1s 2ms/step - loss: 0.0499 - acc: 0.9733\n",
      "Epoch 2/25\n",
      "337/337 [==============================] - 1s 2ms/step - loss: 0.0080 - acc: 0.9970\n",
      "Epoch 3/25\n",
      "337/337 [==============================] - 0s 1ms/step - loss: 6.4209e-04 - acc: 1.0000\n",
      "Epoch 4/25\n",
      "337/337 [==============================] - 1s 1ms/step - loss: 3.3318e-04 - acc: 1.0000\n",
      "Epoch 5/25\n",
      "337/337 [==============================] - 1s 1ms/step - loss: 2.0393e-04 - acc: 1.0000\n",
      "Epoch 6/25\n",
      "337/337 [==============================] - 0s 1ms/step - loss: 1.2857e-04 - acc: 1.0000\n",
      "Epoch 7/25\n",
      "337/337 [==============================] - 1s 2ms/step - loss: 7.9759e-05 - acc: 1.0000\n",
      "Epoch 8/25\n",
      "337/337 [==============================] - 1s 2ms/step - loss: 4.9599e-05 - acc: 1.0000\n",
      "Epoch 9/25\n",
      "337/337 [==============================] - 0s 1ms/step - loss: 3.1661e-05 - acc: 1.0000\n",
      "Epoch 10/25\n",
      "337/337 [==============================] - 1s 2ms/step - loss: 2.0668e-05 - acc: 1.0000\n",
      "Epoch 11/25\n",
      "337/337 [==============================] - 1s 2ms/step - loss: 1.3901e-05 - acc: 1.0000\n",
      "Epoch 12/25\n",
      "337/337 [==============================] - 0s 1ms/step - loss: 9.5639e-06 - acc: 1.0000A: 0s - loss: 1.0280e-05 - acc: 1.\n",
      "Epoch 13/25\n",
      "337/337 [==============================] - 1s 2ms/step - loss: 6.6787e-06 - acc: 1.0000\n",
      "Epoch 14/25\n",
      "337/337 [==============================] - 0s 1ms/step - loss: 4.7245e-06 - acc: 1.0000\n",
      "Epoch 15/25\n",
      "337/337 [==============================] - 0s 1ms/step - loss: 3.3623e-06 - acc: 1.0000\n",
      "Epoch 16/25\n",
      "337/337 [==============================] - 0s 1ms/step - loss: 2.4151e-06 - acc: 1.0000\n",
      "Epoch 17/25\n",
      "337/337 [==============================] - 0s 1ms/step - loss: 1.7282e-06 - acc: 1.0000\n",
      "Epoch 18/25\n",
      "337/337 [==============================] - 1s 2ms/step - loss: 1.2448e-06 - acc: 1.0000\n",
      "Epoch 19/25\n",
      "337/337 [==============================] - 0s 1ms/step - loss: 9.0415e-07 - acc: 1.0000\n",
      "Epoch 20/25\n",
      "337/337 [==============================] - 1s 1ms/step - loss: 6.6308e-07 - acc: 1.0000\n",
      "Epoch 21/25\n",
      "337/337 [==============================] - 0s 1ms/step - loss: 4.9930e-07 - acc: 1.0000\n",
      "Epoch 22/25\n",
      "337/337 [==============================] - 1s 2ms/step - loss: 3.6895e-07 - acc: 1.0000\n",
      "Epoch 23/25\n",
      "337/337 [==============================] - 1s 2ms/step - loss: 2.7220e-07 - acc: 1.0000\n",
      "Epoch 24/25\n",
      "337/337 [==============================] - 1s 2ms/step - loss: 2.2657e-07 - acc: 1.0000\n",
      "Epoch 25/25\n",
      "337/337 [==============================] - 0s 1ms/step - loss: 1.6007e-07 - acc: 1.0000\n",
      "337/337 [==============================] - 0s 109us/step\n",
      "Accuracy = 99.40652822528465 %\n",
      "[2, 420, 253, 65, 11, 15, 421, 81, 22, 17, 636, 6, 5, 162, 637, 254, 82, 1, 20, 255, 4, 422]\n",
      "674/674 [==============================] - 0s 143us/step\n",
      "Accuracy = 93.32344213649851 %\n"
     ]
    }
   ],
   "source": [
    "embedding_size = 8\n",
    "\n",
    "# TODO create and compile model\n",
    "model = Sequential()\n",
    "# Turns positive integers (indexes) into dense vectors of fixed size. eg. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n",
    "model.add(Embedding(vocab_size, embedding_size))\n",
    "model.add(GRU(16))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "# Allow us to see all actors available in dataset\n",
    "actors = unique([l[0] for l in lines])\n",
    "actors_filtered = ['VADER', 'ANAKIN']\n",
    "\n",
    "# Loop on actor on which I wish to train\n",
    "for actor in actors_filtered:\n",
    "    print(\"----------------------------------------------------\")\n",
    "    print(\"---------   Training with \" + actor + \" :       ----\")\n",
    "    print(\"----------------------------------------------------\")\n",
    "\n",
    "    # Get first half data - TRAIN\n",
    "    train_data = lines[:337]\n",
    "    x_train = [t[1] for t in train_data]\n",
    "    y_train = [(1, 0)[t[0] == actor] for t in train_data]\n",
    "    \n",
    "    # Get second half data - TEST\n",
    "    test_data = lines[337:]\n",
    "    x_test = [t[1] for t in test_data]\n",
    "    y_test = [(1, 0)[t[0] == actor] for t in test_data]\n",
    "    \n",
    "    # Generate pad based on train data    \n",
    "    x_train_pad, x_test_pad = getXPad(x_train, x_test)\n",
    "    \n",
    "    # Fit model    \n",
    "    model.fit(x_train_pad, y_train, epochs=25, batch_size=15)\n",
    "    accuracy = model.evaluate(x_test_pad,y_test)\n",
    "    print(\"Accuracy =\", accuracy[1]*100,\"%\")\n",
    "\n",
    "# FINAL TEST - Generate pad based on all data    \n",
    "x_train_pad, x_test_pad = getXPad([t[1] for t in lines],  [t[1] for t in lines])\n",
    "y_test = [(1, 0)[t[0] == 'VADER' or t[0] == 'ANAKIN'] for t in lines]\n",
    "\n",
    "# FINAL TEST - EVALUATE    \n",
    "accuracy = model.evaluate(x_test_pad,y_test)\n",
    "print(\"Accuracy =\", accuracy[1]*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
